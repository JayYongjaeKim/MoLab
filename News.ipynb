{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOKMEY0xlGeh70zQb0zXfj6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JayYongjaeKim/MoLab/blob/main/News.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kKjn90QZTsGR"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tensorflow.keras.datasets import reuters\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing (num_words = None):\n",
        "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=num_words, test_split=0.2)\n",
        "    word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
        "    index_to_word = { index + 3 : word for word, index in word_index.items()}\n",
        "    for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
        "        index_to_word[index]=token\n",
        "\n",
        "    x_train = [' '.join([index_to_word[index] for index in sequence]) for sequence in x_train]\n",
        "    x_test = [' '.join([index_to_word[index] for index in sequence]) for sequence in x_test]\n",
        "\n",
        "    return x_train, x_test, y_train, y_test"
      ],
      "metadata": {
        "id": "NKaMDBiPUuJK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorizer (x_train, x_test):\n",
        "    dtmvector = CountVectorizer()\n",
        "    x_train_dtm = dtmvector.fit_transform(x_train)\n",
        "    x_test_dtm = dtmvector.transform(x_test)\n",
        "\n",
        "    tfidf_transformer = TfidfTransformer()\n",
        "    x_train_tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
        "    x_test_tfidfv = tfidf_transformer.transform(x_test_dtm)\n",
        "\n",
        "    return x_train_tfidfv, x_test_tfidfv"
      ],
      "metadata": {
        "id": "0tEqh6XjU7-G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modeling (x_train, y_train, x_test, y_test):\n",
        "    model = {'Naive Bayes': MultinomialNB(),\n",
        "        'Logistic Regression': LogisticRegression(C=10000, penalty='l2', max_iter=3000),\n",
        "        # 'SVM': LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False),\n",
        "        # 'Decision Tree': DecisionTreeClassifier(max_depth=10, random_state=0),\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=5, random_state=0),\n",
        "        # 'Gradient Boosting': GradientBoostingClassifier(random_state=0),\n",
        "        }\n",
        "\n",
        "    model_results = {}\n",
        "    for name, model in model.items():\n",
        "        model.fit(x_train, y_train)\n",
        "        y_pred = model.predict(x_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        model_results[name] = acc\n",
        "    return model_results"
      ],
      "metadata": {
        "id": "9ZjPw1-xae0M"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_list = [None, 5000, 10000, 20000]\n",
        "results = {}\n",
        "\n",
        "for num_words in num_list:\n",
        "    print({num_words})\n",
        "    x_train, x_test, y_train, y_test = preprocessing(num_words = num_words)\n",
        "    x_train_vec, x_test_vec = vectorizer(x_train, x_test)\n",
        "    model_result = modeling(x_train_vec, y_train, x_test_vec, y_test)\n",
        "    results[num_words] = model_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LSV7Y15bn8t",
        "outputId": "15a9f401-2bed-472f-ff7e-8c827cd6648c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{None}\n",
            "{5000}\n",
            "{10000}\n",
            "{20000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for num_words, model_results in results.items():\n",
        "    print(\"=\"*10)\n",
        "    print(f\"num_words={num_words}:\")\n",
        "    for model_name, accuracy in model_results.items():\n",
        "        print(f\"{model_name}: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOQDs4f7cVKN",
        "outputId": "f1a207df-7ccf-4fdd-a82b-4351b137d39f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========\n",
            "num_words=None:\n",
            "Naive Bayes: 0.5997\n",
            "Logistic Regression: 0.8112\n",
            "Random Forest: 0.6545\n",
            "==========\n",
            "num_words=5000:\n",
            "Naive Bayes: 0.6732\n",
            "Logistic Regression: 0.8059\n",
            "Random Forest: 0.7012\n",
            "==========\n",
            "num_words=10000:\n",
            "Naive Bayes: 0.6567\n",
            "Logistic Regression: 0.8085\n",
            "Random Forest: 0.6741\n",
            "==========\n",
            "num_words=20000:\n",
            "Naive Bayes: 0.6193\n",
            "Logistic Regression: 0.8121\n",
            "Random Forest: 0.6714\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UGYcGRM1ofrw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}